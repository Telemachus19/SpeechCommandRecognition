import librosa
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os
import IPython.display as ipd
import keras
from sklearn.model_selection import train_test_split
import tensorflow as tf
import pickle as pk
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from pathlib import Path
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras import Sequential
from tensorflow.keras.layers import SpatialDropout2D, Conv2D, LeakyReLU, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dense
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam





def extract_mfcc(audio_path, mfcc_max_padding=0, n_mfcc=40):
    """
    Extract MFCC features from an audio file
    Parameters:
    audio_path: str, path to audio file
    Returns:
        mfcc: MFCC features
    """
    try:
        # Load audio file
        audio, sr = librosa.load(audio_path)

        # Normalized Audio between -1 and 1
        normalized_audio = librosa.util.normalize(audio)

        # Extraction of MFCC
        mfcc = librosa.feature.mfcc(
            y = normalized_audio,
            sr = sr,
            n_mfcc = n_mfcc)

        # Normalize MFCC between -1 and 1
        normalized_mfcc = librosa.util.normalize(mfcc)
        
        # If max_padding set to any thing other than zero, then add padding.
        shape = normalized_mfcc.shape[1]
        if (mfcc_max_padding > 0 & shape < mfcc_max_padding):
            xDiff = mfcc_max_padding - shape
            xLeft = xDiff//2
            xRight = xDiff-xLeft
            normalized_mfcc = np.pad(normalized_mfcc, pad_width=((0,0), (xLeft, xRight)), mode='constant')
        
        return normalized_mfcc
    except Exception as e:
        print(f"Error processing {audio_path}: {str(e)}")
        return None


# Given an numpy array of features, zero-pads each ocurrence to max_padding
def add_padding(features, mfcc_max_padding=174):
    padded = []

    # Add padding
    for i in range(len(features)):
        px = features[i]
        size = len(px[0])
        # Add padding if required
        if (size < mfcc_max_padding):
            xDiff = mfcc_max_padding - size
            xLeft = xDiff//2
            xRight = xDiff-xLeft
            px = np.pad(px, pad_width=((0,0), (xLeft, xRight)), mode='constant')
        
        padded.append(px)

    return padded


def prepare_dateset(data_dir):
    """
    Prepare dataset from directory
    Args:
        data_dir: Directory containing audio files in class subdirectories
        max_samples_per_class: Maximum number of samples to use per class
    Returns:
        X: MFCC features
        y: Labels (encoded by LabelEncoder())
        labels: return lables as an array
    """
    data_dir = Path(data_dir)
    features = []
    labels = []
    max_frames = 0
    frames_num = 0
    label_endcoder = LabelEncoder()

    class_dirs = [d for d in data_dir.iterdir() if d.is_dir()]

    for class_dir in class_dirs:
        class_name = class_dir.name
        audio_files = list(class_dir.glob('*.wav'))
        print(f"Processing class: {class_name} - {len(audio_files)} files")
        for audio_file in audio_files:
            mfcc = extract_mfcc(str(audio_file))
            if mfcc is not None:
                features.append(mfcc)
                labels.append(class_name)
                frames_num = mfcc.shape[1]

            if (frames_num > max_frames):
                max_frames = frames_num
    print(max_frames)
    padded_features = add_padding(features, max_frames)
    # label_endcoder.fit(labels)
    # y = label_endcoder.transform(labels)

    return padded_features, labels


nn = extract_mfcc('./aug/bed/noisy_1_00176480_nohash_0.wav')


features, y = prepare_dateset('./aug/')


# Verify shapes
print("Raw features length: {}".format(len(features)))
print("Padded features length: {}".format(len(y)))
# print("Feature labels length: {}".format(len(lables)))


X = np.array(features)
y = np.array(y)


X.shape


y.shape


from keras.utils import to_categorical


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=44, shuffle =True)


X_train.shape


le = LabelEncoder()
y_test_encoded = to_categorical(le.fit_transform(y_test))
y_train_encoded = to_categorical(le.fit_transform(y_train))


num_rows = 40
num_columns = 44 
num_channels = 1


X_train = X_train.reshape(X_train.shape[0], num_rows, num_columns, num_channels)
X_test = X_test.reshape(X_test.shape[0], num_rows, num_columns, num_channels)


X_train.shape


num_labels = y_train_encoded.shape[1]


num_labels


def create_model(spatial_dropout_rate_1=0, spatial_dropout_rate_2=0, l2_rate=0):

    # Create a secquential object
    model = Sequential()


    # Conv 1
    model.add(Conv2D(filters=32, 
                     kernel_size=(3, 3), 
                     kernel_regularizer=l2(l2_rate), 
                     input_shape=(num_rows, num_columns, num_channels)))
    model.add(LeakyReLU(alpha=0.1))
    model.add(BatchNormalization())

    model.add(SpatialDropout2D(spatial_dropout_rate_1))
    model.add(Conv2D(filters=32, 
                     kernel_size=(3, 3), 
                     kernel_regularizer=l2(l2_rate)))
    model.add(LeakyReLU(alpha=0.1))
    model.add(BatchNormalization())


    # Max Pooling #1
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(SpatialDropout2D(spatial_dropout_rate_1))
    model.add(Conv2D(filters=64, 
                     kernel_size=(3, 3), 
                     kernel_regularizer=l2(l2_rate)))
    model.add(LeakyReLU(alpha=0.1))
    model.add(BatchNormalization())

    model.add(SpatialDropout2D(spatial_dropout_rate_2))
    model.add(Conv2D(filters=64, 
                     kernel_size=(3,3), 
                     kernel_regularizer=l2(l2_rate)))
    model.add(LeakyReLU(alpha=0.1))
    model.add(BatchNormalization())
    
   
    # Reduces each h√ów feature map to a single number by taking the average of all h,w values.
    model.add(GlobalAveragePooling2D())


    # Softmax output
    model.add(Dense(num_labels, activation='softmax'))
    
    return model

# Regularization rates
spatial_dropout_rate_1 = 0.07
spatial_dropout_rate_2 = 0.14
l2_rate = 0.0005

model = create_model(spatial_dropout_rate_1, spatial_dropout_rate_2, l2_rate)


adam = Adam(learning_rate=1e-4, beta_1=0.99, beta_2=0.999)
model.compile(
    loss='categorical_crossentropy', 
    metrics=['accuracy'], 
    optimizer=adam)

# Display model architecture summary 
model.summary()


checkpoint = ModelCheckpoint('best_model.keras',
                           monitor='val_accuracy',
                           save_best_only=True,
                           mode='max',
                           verbose=1)
    
early_stopping = EarlyStopping(monitor='val_accuracy',
                             patience=10,
                             restore_best_weights=True,
                             verbose=1)


history = model.fit(X_train, 
                    y_train_encoded, 
                    batch_size=32, 
                    epochs=10, 
                    validation_split=1/12.,
                    callbacks=[checkpoint, early_stopping], 
                    verbose=1)


model.evaluate(X_test, y_test_encoded)


model.predict(X_test)


print('X_train shape is ' , X_train.shape)
print('X_test shape is ' , X_test.shape)
print('y_train shape is ' , y_train.shape)
print('y_test shape is ' , y_test.shape)


num_class=len(pd.unique(label))
model=keras.Sequential()
model.add(keras.layers.Conv1D(filters=8, kernel_size=13,activation=tf.nn.relu,input_shape=(16000,1)))
model.add(keras.layers.MaxPooling1D(3))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Conv1D(filters=16, kernel_size=11,activation=tf.nn.relu))
model.add(keras.layers.MaxPooling1D(3))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Conv1D(filters=32, kernel_size=9,activation=tf.nn.relu))
model.add(keras.layers.MaxPooling1D(3))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Conv1D(filters=64, kernel_size=7,activation=tf.nn.relu))
model.add(keras.layers.MaxPooling1D(3))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(256,activation=tf.nn.relu))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Dense(128,activation=tf.nn.relu))
model.add(keras.layers.Dropout(.3))
model.add(keras.layers.Dense(num_class,activation=tf.nn.softmax))
